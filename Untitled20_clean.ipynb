{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/parhamalikhan/Credit-Card-Fraud-Detection/blob/main/Untitled20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PDKrF9XiIlK",
    "outputId": "acfbdd2e-821e-4b1c-a2b5-40052abe46d7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "REAL_DIR = \"/content/drive/MyDrive/Master's and PhD applications for INDIMA\"\n",
    "AI_DIR   = \"/content/drive/MyDrive/ai generated\"\n",
    "\n",
    "print(\"REAL exists?\", Path(REAL_DIR).exists(), \"| is_dir?\", Path(REAL_DIR).is_dir())\n",
    "print(\"AI   exists?\", Path(AI_DIR).exists(),   \"| is_dir?\", Path(AI_DIR).is_dir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9PBJgZ2jm4r",
    "outputId": "c2a595dc-13b5-4b38-c0ab-8589bbc20ea4"
   },
   "outputs": [],
   "source": [
    "!pip -q uninstall -y fitz pymupdf PyMuPDF > /dev/null 2>&1\n",
    "!pip -q install \"PyMuPDF==1.26.7\" \"pillow<12\" pytesseract tqdm scikit-learn joblib\n",
    "!apt-get -y install tesseract-ocr > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQ1B1FAqkIHy",
    "outputId": "068e1bf7-404f-44ba-ad0e-8cff72d9c4ec"
   },
   "outputs": [],
   "source": [
    "import re, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "ALLOWED_EXT = {\".pdf\", \".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\"}\n",
    "\n",
    "# Extraction config\n",
    "OCR_PDF_PAGES     = 5\n",
    "MIN_DIGITAL_CHARS = 80\n",
    "PDF_RENDER_DPI    = 250  # try 300 if OCR weak\n",
    "\n",
    "print(\"PyMuPDF OK:\", fitz.__doc__[:60])\n",
    "print(\"Tesseract:\", pytesseract.get_tesseract_version())\n",
    "print(\"Pillow:\", Image.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OCZCLagkPlj",
    "outputId": "b598dbb7-ec5b-4e70-f48a-1c2e3d696e0a"
   },
   "outputs": [],
   "source": [
    "def discover_files(root_dir: str):\n",
    "    root = Path(root_dir)\n",
    "    out = []\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in ALLOWED_EXT:\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "def infer_domain_from_path(file_path: str, root_dir: str) -> str:\n",
    "    try:\n",
    "        rp = Path(file_path).resolve()\n",
    "        rr = Path(root_dir).resolve()\n",
    "        rel = rp.relative_to(rr)\n",
    "        return rel.parts[0] if len(rel.parts) else \"root\"\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "def infer_doc_type_from_path(p: str) -> str:\n",
    "    s = p.lower()\n",
    "    if \"handbook\" in s:\n",
    "        return \"handbook\"\n",
    "    if \"transcript\" in s:\n",
    "        return \"transcript\"\n",
    "    if \"degree\" in s or \"certificate\" in s or \"diploma\" in s:\n",
    "        return \"degree\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def build_df(real_dir: str, ai_dir: str) -> pd.DataFrame:\n",
    "    real_files = discover_files(real_dir)\n",
    "    ai_files   = discover_files(ai_dir)\n",
    "\n",
    "    rows = []\n",
    "    for p in real_files:\n",
    "        rows.append({\n",
    "            \"path\": str(p),\n",
    "            \"y\": 0,\n",
    "            \"file_type\": p.suffix.lower().lstrip(\".\"),\n",
    "            \"source_dir\": \"REAL\",\n",
    "            \"domain\": infer_domain_from_path(str(p), real_dir),\n",
    "            \"doc_type\": infer_doc_type_from_path(str(p)),\n",
    "        })\n",
    "    for p in ai_files:\n",
    "        rows.append({\n",
    "            \"path\": str(p),\n",
    "            \"y\": 1,\n",
    "            \"file_type\": p.suffix.lower().lstrip(\".\"),\n",
    "            \"source_dir\": \"AI\",\n",
    "            \"domain\": infer_domain_from_path(str(p), ai_dir),\n",
    "            \"doc_type\": infer_doc_type_from_path(str(p)),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = build_df(REAL_DIR, AI_DIR)\n",
    "\n",
    "print(\"DF shape:\", df.shape)\n",
    "print(df.head(3))\n",
    "print(\"\\nfile_type counts:\\n\", df[\"file_type\"].value_counts(dropna=False))\n",
    "print(\"\\ndoc_type x y:\\n\", pd.crosstab(df[\"doc_type\"], df[\"y\"], dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EN_FNOO7Q1zX",
    "outputId": "be4cebac-2253-4b93-b5f5-011b61c78e0d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "# PyMuPDF + OCR imports\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "OCR_PDF_PAGES     = 5\n",
    "MIN_DIGITAL_CHARS = 80\n",
    "PDF_RENDER_DPI    = 250\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.replace(\"\\x00\", \" \")\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "    return t.strip()\n",
    "\n",
    "def word_count(text: str) -> int:\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(re.findall(r\"\\w+\", text))\n",
    "\n",
    "def extract_pdf_digital(pdf_path: str) -> str:\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        parts = [(page.get_text(\"text\") or \"\") for page in doc]\n",
    "        return normalize_text(\"\\n\".join(parts))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def ocr_image_pil(img) -> str:\n",
    "    try:\n",
    "        config = \"--oem 3 --psm 6\"\n",
    "        return normalize_text(pytesseract.image_to_string(img, config=config))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def ocr_pdf_first_pages(pdf_path: str, max_pages=5, dpi=250) -> str:\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        n = min(len(doc), max_pages)\n",
    "        out = []\n",
    "        for i in range(n):\n",
    "            pix = doc[i].get_pixmap(dpi=dpi)\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "            out.append(ocr_image_pil(img))\n",
    "        return normalize_text(\"\\n\".join([x for x in out if x]))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_one(path: str, file_type: str):\n",
    "    ft = (file_type or \"\").lower()\n",
    "\n",
    "    if ft in {\"png\",\"jpg\",\"jpeg\",\"tif\",\"tiff\"}:\n",
    "        try:\n",
    "            img = Image.open(path)\n",
    "            txt = ocr_image_pil(img)\n",
    "            wc = word_count(txt)\n",
    "            if wc == 0:\n",
    "                return \"\", \"none\", True, 0, \"image_ocr_empty\"\n",
    "            return txt, \"ocr\", True, wc, \"\"\n",
    "        except Exception as e:\n",
    "            return \"\", \"none\", True, 0, f\"image_exception:{type(e).__name__}\"\n",
    "\n",
    "    if ft == \"pdf\":\n",
    "        digital = extract_pdf_digital(path)\n",
    "        if len(digital) >= MIN_DIGITAL_CHARS:\n",
    "            wc = word_count(digital)\n",
    "            return digital, \"digital\", False, wc, \"\"\n",
    "\n",
    "        ocr_txt = ocr_pdf_first_pages(path, max_pages=OCR_PDF_PAGES, dpi=PDF_RENDER_DPI)\n",
    "        wc = word_count(ocr_txt)\n",
    "        if wc == 0:\n",
    "            return \"\", \"none\", True, 0, \"pdf_digital_low_and_ocr_empty\"\n",
    "        return ocr_txt, \"ocr\", True, wc, \"\"\n",
    "\n",
    "    return \"\", \"none\", False, 0, \"unsupported_file_type\"\n",
    "\n",
    "def run_extraction(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_out = df_in.copy()\n",
    "    for c in [\"doc_text\",\"text_source\",\"is_scanned\",\"word_count\",\"drop_reason\"]:\n",
    "        if c not in df_out.columns:\n",
    "            df_out[c] = None\n",
    "\n",
    "    for i, row in tqdm(df_out.iterrows(), total=len(df_out), desc=\"Extracting\"):\n",
    "        txt, src, scanned, wc, reason = extract_text_one(row[\"path\"], row[\"file_type\"])\n",
    "        df_out.at[i, \"doc_text\"] = txt\n",
    "        df_out.at[i, \"text_source\"] = src\n",
    "        df_out.at[i, \"is_scanned\"] = bool(scanned)\n",
    "        df_out.at[i, \"word_count\"] = int(wc)\n",
    "        df_out.at[i, \"drop_reason\"] = reason or \"\"\n",
    "    return df_out\n",
    "\n",
    "print(\"✅ run_extraction is defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xftiq571k8rN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d96c7a3d-a67d-4840-a37d-0f2b69d2f5f6"
   },
   "outputs": [],
   "source": [
    "cols = [\"doc_text\",\"text_source\",\"is_scanned\",\"word_count\",\"drop_reason\"]\n",
    "\n",
    "df = run_extraction(df)\n",
    "\n",
    "print(df[\"text_source\"].value_counts(dropna=False))\n",
    "print(df[\"drop_reason\"].value_counts(dropna=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJpH37gjZN_m",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9200a490-db02-4b32-a197-1b73f120a22b"
   },
   "outputs": [],
   "source": [
    "# If doc_type exists (it does in your df), we finalize doc_type_final:\n",
    "df[\"doc_type_final\"] = df[\"doc_type\"].copy()\n",
    "df.loc[df[\"doc_type_final\"]==\"unknown\", \"doc_type_final\"] = \"degree\"\n",
    "\n",
    "print(\"doc_type_final counts:\\n\", df[\"doc_type_final\"].value_counts(dropna=False))\n",
    "print(\"\\ndoc_type_final x y:\\n\", pd.crosstab(df[\"doc_type_final\"], df[\"y\"], dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2IR6smeZOuD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ca4d7ab0-deb3-4f66-d20f-996d0c96c67d"
   },
   "outputs": [],
   "source": [
    "print(\"TOTAL files:\", len(df))\n",
    "print(\"\\ntext_source:\\n\", df[\"text_source\"].value_counts(dropna=False))\n",
    "print(\"\\nword_count:\\n\", df[\"word_count\"].describe(percentiles=[.1,.25,.5,.75,.9]))\n",
    "print(\"\\ndrop_reason top:\\n\", df[\"drop_reason\"].value_counts(dropna=False).head(20))\n",
    "\n",
    "# Debug export (never lose \"why we got fewer docs\")\n",
    "df_debug_none = df[df[\"text_source\"]==\"none\"][[\"path\",\"y\",\"file_type\",\"source_dir\",\"domain\",\"doc_type_final\",\"drop_reason\",\"word_count\"]].copy()\n",
    "df_debug_none.to_csv(\"dropped_files_debug.csv\", index=False)\n",
    "print(\"\\nSaved: dropped_files_debug.csv | rows:\", len(df_debug_none))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uJ9J2wMZRVW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4c8038ab-309f-4f1c-cfea-0bebe2426693"
   },
   "outputs": [],
   "source": [
    "MIN_WORDS = 10  # start with 10; later you can try 30\n",
    "\n",
    "df_degree_all  = df[df[\"doc_type_final\"]==\"degree\"].copy()\n",
    "df_degree_text = df_degree_all[df_degree_all[\"word_count\"].fillna(0) >= MIN_WORDS].copy()\n",
    "\n",
    "print(\"degree_all :\", df_degree_all.shape)\n",
    "print(\"degree_text:\", df_degree_text.shape)\n",
    "\n",
    "print(\"\\ny counts (degree_text):\\n\", df_degree_text[\"y\"].value_counts(dropna=False))\n",
    "print(\"\\ntext_source (degree_text):\\n\", df_degree_text[\"text_source\"].value_counts(dropna=False))\n",
    "\n",
    "if len(df_degree_text) == 0:\n",
    "    raise ValueError(\"No degree samples with enough text. Lower MIN_WORDS or improve OCR.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDqJloYkZtDT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cda6fa75-cfeb-4b81-c663-a2ca2fb700b6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fix dtype issues that cause the FutureWarning\n",
    "df_degree_all[\"word_count\"]  = pd.to_numeric(df_degree_all[\"word_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df_degree_text[\"word_count\"] = pd.to_numeric(df_degree_text[\"word_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Make sure domain exists\n",
    "if \"domain\" not in df_degree_text.columns:\n",
    "    df_degree_text[\"domain\"] = \"unknown\"\n",
    "\n",
    "print(\"degree_all:\", df_degree_all.shape, \"| text:\", df_degree_text.shape)\n",
    "print(\"domain unique:\", df_degree_text[\"domain\"].nunique())\n",
    "print(pd.crosstab(df_degree_text[\"text_source\"], df_degree_text[\"y\"], normalize=\"columns\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWqh405gbneu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
    "\n",
    "def threshold_at_fpr(y_true, y_score, target_fpr=0.05):\n",
    "    y_true = np.asarray(y_true); y_score = np.asarray(y_score)\n",
    "    neg = y_score[y_true == 0]\n",
    "    return float(np.quantile(neg, 1 - target_fpr)) if len(neg) else 0.5\n",
    "\n",
    "def groupkfold_oof_proba(model, X, y, groups, n_splits=5):\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    oof = np.zeros(len(y), dtype=float)\n",
    "    for tr, te in gkf.split(X, y, groups=groups):\n",
    "        model.fit(X[tr], y[tr])\n",
    "        oof[te] = model.predict_proba(X[te])[:, 1]\n",
    "    return oof\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOujNR_wb21p"
   },
   "source": [
    "MODEL 1 — TF-IDF (word+char) + LR with domain-holdout CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQnNw7wJdfN8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8637e559-40c8-44a0-8264-a71ebcddfc66"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dom = df_degree_text.groupby(\"domain\")[\"y\"].agg(\n",
    "    n=\"count\",\n",
    "    n_ai=\"sum\",\n",
    "    n_human=lambda s: (s==0).sum()\n",
    ").sort_values([\"n_human\",\"n_ai\"], ascending=True)\n",
    "\n",
    "print(dom.head(30))\n",
    "print(\"\\n#domains:\", dom.shape[0])\n",
    "print(\"#single-class domains:\", ( (dom[\"n_human\"]==0) | (dom[\"n_ai\"]==0) ).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCIhJyOzb9xt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "da4be3b6-2897-45ae-b3f5-fa941195215f"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "REAL_ROOT = Path(REAL_DIR).resolve()\n",
    "AI_ROOT   = Path(AI_DIR).resolve()\n",
    "\n",
    "def group_id_from_path(path_str: str, source_dir: str) -> str:\n",
    "    p = Path(path_str).resolve()\n",
    "\n",
    "    # REAL: group by student folder (best leakage control)\n",
    "    if source_dir == \"REAL\":\n",
    "        try:\n",
    "            rel = p.relative_to(REAL_ROOT)\n",
    "            parts = list(rel.parts)\n",
    "        except Exception:\n",
    "            parts = list(p.parts)\n",
    "\n",
    "        # pick first folder that looks like studentXXX\n",
    "        for part in parts:\n",
    "            if re.match(r\"(?i)^student\", part):\n",
    "                return f\"REAL_{part}\"\n",
    "        # fallback: parent folder name\n",
    "        return f\"REAL_{p.parent.name}\"\n",
    "\n",
    "    # AI: group by CERT timestamp/batch if present\n",
    "    if source_dir == \"AI\":\n",
    "        m = re.search(r\"CERT-(\\d+)\", p.name)\n",
    "        if m:\n",
    "            return f\"AI_CERT_{m.group(1)}\"\n",
    "        # fallback: parent folder\n",
    "        return f\"AI_{p.parent.name}\"\n",
    "\n",
    "    return \"unknown\"\n",
    "\n",
    "df_degree_text[\"group_id\"] = [\n",
    "    group_id_from_path(p, s) for p, s in zip(df_degree_text[\"path\"], df_degree_text[\"source_dir\"])\n",
    "]\n",
    "\n",
    "print(\"group_id unique:\", df_degree_text[\"group_id\"].nunique())\n",
    "tmp = df_degree_text.groupby(\"group_id\")[\"y\"].agg(n=\"count\", n_ai=\"sum\", n_human=lambda x: (x==0).sum())\n",
    "print(\"single-class groups:\", ((tmp[\"n_ai\"]==0) | (tmp[\"n_human\"]==0)).sum(), \" / \", tmp.shape[0])\n",
    "print(tmp.sort_values(\"n\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "PAD = \"zzpadzz\"\n",
    "print(\"✅ PAD defined:\", PAD)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZG74UOJFzFJ",
    "outputId": "318a7bf2-c537-4928-950e-0939187685e3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# uses your PAD token (zzpadzz) from fixed-length step\n",
    "tfidf_feats = FeatureUnion([\n",
    "    (\"word\", TfidfVectorizer(\n",
    "        max_features=20000,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=2,\n",
    "        binary=True,\n",
    "        stop_words=[PAD],   # ignore padding token\n",
    "    )),\n",
    "    (\"char\", TfidfVectorizer(\n",
    "        analyzer=\"char_wb\",\n",
    "        ngram_range=(3,5),\n",
    "        min_df=2,\n",
    "        binary=True,\n",
    "    )),\n",
    "])\n",
    "\n",
    "tfidf_lr = Pipeline([\n",
    "    (\"tfidf\", tfidf_feats),\n",
    "    (\"lr\", LogisticRegression(max_iter=4000))\n",
    "])\n",
    "\n",
    "print(\"✅ tfidf_lr is defined\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8by9nE4fEU5u",
    "outputId": "683c33d2-7cc6-4780-f561-8f3904a0b44e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AG-lZSBg7mR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7980f12b-1e37-461f-ed22-53f5d41bf283"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
    "\n",
    "X = df_degree_text[\"doc_text\"].fillna(\"\").astype(str).values\n",
    "y = df_degree_text[\"y\"].astype(int).values\n",
    "groups = df_degree_text[\"group_id\"].astype(str).values\n",
    "\n",
    "# your existing tfidf_lr pipeline is assumed defined\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "for tr, te in sgkf.split(X, y, groups=groups):\n",
    "    tfidf_lr.fit(X[tr], y[tr])\n",
    "    oof[te] = tfidf_lr.predict_proba(X[te])[:,1]\n",
    "\n",
    "print(\"TFIDF StratifiedGroupKFold ROC-AUC:\", roc_auc_score(y, oof))\n",
    "print(\"TFIDF StratifiedGroupKFold PR-AUC :\", average_precision_score(y, oof))\n",
    "\n",
    "def threshold_at_fpr(y_true, y_score, target_fpr=0.05):\n",
    "    y_true = np.asarray(y_true); y_score = np.asarray(y_score)\n",
    "    neg = y_score[y_true==0]\n",
    "    return float(np.quantile(neg, 1-target_fpr)) if len(neg) else 0.5\n",
    "\n",
    "thr = threshold_at_fpr(y, oof, 0.05)\n",
    "pred = (oof >= thr).astype(int)\n",
    "\n",
    "print(\"thr (~5% FPR):\", thr)\n",
    "print(\"Confusion:\\n\", confusion_matrix(y, pred))\n",
    "print(classification_report(y, pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4zCM7UPhw0l"
   },
   "source": [
    "Fit final TFIDF model + save (deployment model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3SNP3_fhGIX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "42c1ca63-abb3-4e62-957d-b0c740e9d785"
   },
   "outputs": [],
   "source": [
    "import joblib, json\n",
    "\n",
    "X_all = df_degree_text[\"doc_text\"].fillna(\"\").astype(str).values\n",
    "y_all = df_degree_text[\"y\"].astype(int).values\n",
    "\n",
    "tfidf_lr.fit(X_all, y_all)\n",
    "joblib.dump(tfidf_lr, \"tfidf_lr_degree.joblib\")\n",
    "json.dump(\n",
    "    {\"min_words\": MIN_WORDS, \"thr_fpr5\": float(thr), \"eval\": \"StratifiedGroupKFold(group_id)\"},\n",
    "    open(\"tfidf_lr_degree_meta.json\",\"w\"),\n",
    "    indent=2\n",
    ")\n",
    "print(\"Saved final TFIDF: tfidf_lr_degree.joblib + meta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7Zojx_9jViI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def grouped_oof_proba(model, X, y, groups, n_splits=5):\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof = np.zeros(len(y), dtype=float)\n",
    "    for tr, te in sgkf.split(X, y, groups=groups):\n",
    "        model.fit(X[tr], y[tr])\n",
    "        oof[te] = model.predict_proba(X[te])[:,1]\n",
    "    return oof\n",
    "\n",
    "def report(name, y, proba):\n",
    "    print(name)\n",
    "    print(\"  ROC-AUC:\", roc_auc_score(y, proba))\n",
    "    print(\"  PR-AUC :\", average_precision_score(y, proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8iJXNRXyn5G",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eaa4a784-7aa4-423e-92a1-8477752787b4"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "meta = df_degree_text.copy()\n",
    "meta[\"file_type\"] = meta[\"file_type\"].astype(str)\n",
    "meta[\"text_source\"] = meta[\"text_source\"].astype(str)\n",
    "\n",
    "X_meta = meta[[\"word_count\",\"file_type\",\"text_source\"]]\n",
    "y = meta[\"y\"].astype(int).values\n",
    "groups = meta[\"group_id\"].astype(str).values\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"file_type\",\"text_source\"]),\n",
    "        (\"num\", \"passthrough\", [\"word_count\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "meta_lr = Pipeline([(\"pre\", pre), (\"lr\", LogisticRegression(max_iter=5000))])\n",
    "\n",
    "# grouped CV\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "for tr, te in sgkf.split(X_meta, y, groups=groups):\n",
    "    meta_lr.fit(X_meta.iloc[tr], y[tr])\n",
    "    oof[te] = meta_lr.predict_proba(X_meta.iloc[te])[:,1]\n",
    "\n",
    "report(\"META-ONLY (word_count + file_type + text_source)\", y, oof)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUnztZatyrk9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9b93bcb0-1911-4b09-832c-6f48582f5073"
   },
   "outputs": [],
   "source": [
    "df_degree_ocr = df_degree_text[df_degree_text[\"text_source\"]==\"ocr\"].copy()\n",
    "print(df_degree_ocr.shape, df_degree_ocr[\"y\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3z1YVImkzXmj"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_ocr_text(t: str) -> str:\n",
    "    t = t or \"\"\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"\\b\\d{4,}\\b\", \" <num> \", t)     # remove long IDs / years / serials\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "df_degree_text[\"doc_text_clean\"] = df_degree_text[\"doc_text\"].apply(clean_ocr_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64bTovhy06oh"
   },
   "source": [
    "OCR-only extraction for ALL degree files (no digital extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMMsxpGRzfov"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import fitz\n",
    "\n",
    "OCR_PDF_PAGES = 3      # start with 3 (faster); try 5 if needed\n",
    "PDF_RENDER_DPI = 250\n",
    "MAX_WORDS = 400        # length normalization (removes word_count shortcut)\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.replace(\"\\x00\", \" \")\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def clean_text_for_fairness(t: str) -> str:\n",
    "    # remove obvious IDs / serials (optional but helps)\n",
    "    t = (t or \"\").lower()\n",
    "    t = re.sub(r\"\\b\\d{4,}\\b\", \" <num> \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def truncate_words(t: str, max_words: int) -> str:\n",
    "    w = (t or \"\").split()\n",
    "    return \" \".join(w[:max_words])\n",
    "\n",
    "def ocr_image_pil(img: Image.Image) -> str:\n",
    "    config = \"--oem 3 --psm 6\"\n",
    "    return normalize_text(pytesseract.image_to_string(img, config=config))\n",
    "\n",
    "def ocr_pdf_pages(pdf_path: str, max_pages=3, dpi=250) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    n = min(len(doc), max_pages)\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        pix = doc[i].get_pixmap(dpi=dpi)\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        out.append(ocr_image_pil(img))\n",
    "    return normalize_text(\" \".join([x for x in out if x]))\n",
    "\n",
    "def extract_ocr_only(path: str, file_type: str):\n",
    "    ft = (file_type or \"\").lower()\n",
    "    try:\n",
    "        if ft == \"pdf\":\n",
    "            t = ocr_pdf_pages(path, max_pages=OCR_PDF_PAGES, dpi=PDF_RENDER_DPI)\n",
    "        elif ft in {\"png\",\"jpg\",\"jpeg\",\"tif\",\"tiff\"}:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            t = ocr_image_pil(img)\n",
    "        else:\n",
    "            return \"\", \"none\", 0, \"unsupported_file_type\"\n",
    "\n",
    "        t = clean_text_for_fairness(t)\n",
    "        t = truncate_words(t, MAX_WORDS)\n",
    "\n",
    "        wc = len(re.findall(r\"\\w+\", t))\n",
    "        if wc == 0:\n",
    "            return \"\", \"none\", 0, \"ocr_empty\"\n",
    "        return t, \"ocr\", wc, \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"\", \"none\", 0, f\"ocr_exception:{type(e).__name__}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGCap_VH0-MA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4f2bfdb0-dc5e-476d-b8e7-cceadaaf516d"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df_degree_all = df[df[\"doc_type_final\"]==\"degree\"].copy()\n",
    "\n",
    "df_degree_all[\"doc_text_ocrall\"] = \"\"\n",
    "df_degree_all[\"text_source_ocrall\"] = \"none\"\n",
    "df_degree_all[\"word_count_ocrall\"] = 0\n",
    "df_degree_all[\"drop_reason_ocrall\"] = \"\"\n",
    "\n",
    "for i, row in tqdm(df_degree_all.iterrows(), total=len(df_degree_all), desc=\"OCR-all degrees\"):\n",
    "    t, src, wc, reason = extract_ocr_only(row[\"path\"], row[\"file_type\"])\n",
    "    df_degree_all.at[i, \"doc_text_ocrall\"] = t\n",
    "    df_degree_all.at[i, \"text_source_ocrall\"] = src\n",
    "    df_degree_all.at[i, \"word_count_ocrall\"] = int(wc)\n",
    "    df_degree_all.at[i, \"drop_reason_ocrall\"] = reason\n",
    "\n",
    "print(df_degree_all[\"text_source_ocrall\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4DGpVrc1AhH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f4fcd76e-5adf-4a01-f9bf-b820486979b7"
   },
   "outputs": [],
   "source": [
    "MIN_WORDS_OCRALL = 10\n",
    "\n",
    "df_degree_text_ocrall = df_degree_all[df_degree_all[\"word_count_ocrall\"] >= MIN_WORDS_OCRALL].copy()\n",
    "\n",
    "print(\"degree_all:\", df_degree_all.shape)\n",
    "print(\"degree_text_ocrall:\", df_degree_text_ocrall.shape)\n",
    "print(df_degree_text_ocrall[\"y\"].value_counts())\n",
    "print(pd.crosstab(df_degree_text_ocrall[\"file_type\"], df_degree_text_ocrall[\"y\"], normalize=\"columns\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SigbJ7Go632p",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "376dcf6d-b130-4165-ed6e-735e95752ba6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "REAL_ROOT = Path(REAL_DIR).resolve()\n",
    "AI_ROOT   = Path(AI_DIR).resolve()\n",
    "\n",
    "def group_id_from_path(path_str: str, source_dir: str) -> str:\n",
    "    p = Path(path_str).resolve()\n",
    "\n",
    "    if source_dir == \"REAL\":\n",
    "        try:\n",
    "            rel = p.relative_to(REAL_ROOT)\n",
    "            parts = list(rel.parts)\n",
    "        except Exception:\n",
    "            parts = list(p.parts)\n",
    "\n",
    "        for part in parts:\n",
    "            if re.match(r\"(?i)^student\", part):\n",
    "                return f\"REAL_{part}\"\n",
    "        return f\"REAL_{p.parent.name}\"\n",
    "\n",
    "    if source_dir == \"AI\":\n",
    "        m = re.search(r\"CERT-(\\d+)\", p.name)\n",
    "        if m:\n",
    "            return f\"AI_CERT_{m.group(1)}\"\n",
    "        return f\"AI_{p.parent.name}\"\n",
    "\n",
    "    return \"unknown\"\n",
    "\n",
    "def ensure_group_id(df_):\n",
    "    if df_ is None or len(df_) == 0:\n",
    "        return df_\n",
    "    if \"group_id\" not in df_.columns:\n",
    "        df_[\"group_id\"] = [\n",
    "            group_id_from_path(p, s) for p, s in zip(df_[\"path\"], df_[\"source_dir\"])\n",
    "        ]\n",
    "    return df_\n",
    "\n",
    "df_degree_text = ensure_group_id(df_degree_text)\n",
    "df_degree_all  = ensure_group_id(df_degree_all) if \"df_degree_all\" in globals() else df_degree_all\n",
    "df_degree_text_ocrall = ensure_group_id(df_degree_text_ocrall) if \"df_degree_text_ocrall\" in globals() else df_degree_text_ocrall\n",
    "\n",
    "print(\"group_id added.\")\n",
    "print(\"df_degree_text has group_id?\", \"group_id\" in df_degree_text.columns)\n",
    "if \"df_degree_text_ocrall\" in globals():\n",
    "    print(\"df_degree_text_ocrall has group_id?\", \"group_id\" in df_degree_text_ocrall.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtAqQ7pr7kDb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9f038c9b-6963-4d44-a037-3122a3811b78"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "meta = df_degree_text_ocrall.copy()\n",
    "meta[\"file_type\"] = meta[\"file_type\"].astype(str)\n",
    "\n",
    "X_meta = meta[[\"word_count_ocrall\",\"file_type\"]]\n",
    "y = meta[\"y\"].astype(int).values\n",
    "groups = meta[\"group_id\"].astype(str).values\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"file_type\"]),\n",
    "    (\"num\", \"passthrough\", [\"word_count_ocrall\"]),\n",
    "])\n",
    "\n",
    "meta_lr = Pipeline([(\"pre\", pre), (\"lr\", LogisticRegression(max_iter=5000))])\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "\n",
    "for tr, te in sgkf.split(X_meta, y, groups=groups):\n",
    "    meta_lr.fit(X_meta.iloc[tr], y[tr])\n",
    "    oof[te] = meta_lr.predict_proba(X_meta.iloc[te])[:,1]\n",
    "\n",
    "print(\"META-only (word_count_ocrall + file_type) ROC-AUC:\", roc_auc_score(y, oof))\n",
    "print(\"META-only PR-AUC:\", average_precision_score(y, oof))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEYZRGAZ7m-8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5ce4c708-e060-49cb-ee04-c2992ab058f3"
   },
   "outputs": [],
   "source": [
    "df_pdf = df_degree_text_ocrall[df_degree_text_ocrall[\"file_type\"]==\"pdf\"].copy()\n",
    "print(\"PDF-only:\", df_pdf.shape)\n",
    "print(df_pdf[\"y\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKsb6tsx8DE-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "950bee9b-d240-41b1-f0a3-f8e6fbfe4ca2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "if \"group_id\" not in df.columns:\n",
    "    REAL_ROOT = Path(REAL_DIR).resolve()\n",
    "    AI_ROOT   = Path(AI_DIR).resolve()\n",
    "\n",
    "    def group_id_from_path(path_str: str, source_dir: str) -> str:\n",
    "        p = Path(path_str).resolve()\n",
    "        if source_dir == \"REAL\":\n",
    "            try:\n",
    "                rel = p.relative_to(REAL_ROOT)\n",
    "                parts = list(rel.parts)\n",
    "            except Exception:\n",
    "                parts = list(p.parts)\n",
    "            for part in parts:\n",
    "                if re.match(r\"(?i)^student\", part):\n",
    "                    return f\"REAL_{part}\"\n",
    "            return f\"REAL_{p.parent.name}\"\n",
    "        if source_dir == \"AI\":\n",
    "            m = re.search(r\"CERT-(\\d+)\", p.name)\n",
    "            if m:\n",
    "                return f\"AI_CERT_{m.group(1)}\"\n",
    "            return f\"AI_{p.parent.name}\"\n",
    "        return \"unknown\"\n",
    "\n",
    "    df[\"group_id\"] = [group_id_from_path(p, s) for p, s in zip(df[\"path\"], df[\"source_dir\"])]\n",
    "\n",
    "print(\"group_id exists?\", \"group_id\" in df.columns, \"| unique:\", df[\"group_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZ-LNAP89X4c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fbebb21f-90b6-403e-c9db-f36cdaab2ae3"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import fitz\n",
    "\n",
    "RENDER_DIR = Path(\"degree_proxy_images\")\n",
    "RENDER_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def make_proxy_image(path_str: str, file_type: str, dpi=200) -> str:\n",
    "    p = Path(path_str)\n",
    "    out = RENDER_DIR / (p.stem + \"_p0.png\")\n",
    "    if out.exists():\n",
    "        return str(out)\n",
    "\n",
    "    ft = (file_type or \"\").lower()\n",
    "    try:\n",
    "        if ft == \"pdf\":\n",
    "            doc = fitz.open(str(p))\n",
    "            if len(doc) == 0:\n",
    "                return \"\"\n",
    "            pix = doc[0].get_pixmap(dpi=dpi)\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "            img.save(out)\n",
    "            return str(out)\n",
    "\n",
    "        if ft in {\"png\",\"jpg\",\"jpeg\",\"tif\",\"tiff\"}:\n",
    "            img = Image.open(str(p)).convert(\"RGB\")\n",
    "            img.save(out)\n",
    "            return str(out)\n",
    "\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "deg_all = df[df[\"doc_type_final\"]==\"degree\"].copy()\n",
    "\n",
    "if \"proxy_img_path\" not in deg_all.columns:\n",
    "    deg_all[\"proxy_img_path\"] = \"\"\n",
    "\n",
    "if (deg_all[\"proxy_img_path\"] == \"\").any():\n",
    "    deg_all[\"proxy_img_path\"] = [\n",
    "        (pp if isinstance(pp, str) and pp else make_proxy_image(p, ft, dpi=200))\n",
    "        for p, ft, pp in zip(deg_all[\"path\"], deg_all[\"file_type\"], deg_all[\"proxy_img_path\"])\n",
    "    ]\n",
    "\n",
    "print(\"deg_all:\", deg_all.shape, \"| proxy empty:\", (deg_all[\"proxy_img_path\"]==\"\").sum())\n",
    "deg_all = deg_all[deg_all[\"proxy_img_path\"]!=\"\"].copy()\n",
    "print(\"deg_all usable:\", deg_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1PaUtPP9dOo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "45ad9364-7637-44bf-92fb-bae73089070d"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Require your extract_ocr_only to exist\n",
    "assert \"extract_ocr_only\" in globals(), \"extract_ocr_only() is not defined in this runtime.\"\n",
    "\n",
    "for col, default in [\n",
    "    (\"doc_text_norm\",\"\"),\n",
    "    (\"word_count_norm\",0),\n",
    "    (\"drop_reason_norm\",\"\"),\n",
    "]:\n",
    "    if col not in deg_all.columns:\n",
    "        deg_all[col] = default\n",
    "\n",
    "need = (deg_all[\"word_count_norm\"].fillna(0).astype(int) == 0) & (deg_all[\"doc_text_norm\"].fillna(\"\") == \"\")\n",
    "print(\"Need OCR-normalization rows:\", int(need.sum()))\n",
    "\n",
    "for i, row in tqdm(deg_all[need].iterrows(), total=int(need.sum()), desc=\"OCR normalized degrees\"):\n",
    "    # we call your existing OCR extractor on proxy images\n",
    "    t, src, wc, reason = extract_ocr_only(row[\"proxy_img_path\"], \"png\")\n",
    "    deg_all.at[i, \"doc_text_norm\"] = t\n",
    "    deg_all.at[i, \"word_count_norm\"] = int(wc)\n",
    "    deg_all.at[i, \"drop_reason_norm\"] = reason or \"\"\n",
    "\n",
    "MIN_WORDS_NORM = 10\n",
    "deg_text_norm = deg_all[deg_all[\"word_count_norm\"].fillna(0).astype(int) >= MIN_WORDS_NORM].copy()\n",
    "\n",
    "print(\"deg_text_norm:\", deg_text_norm.shape)\n",
    "print(\"y counts:\\n\", deg_text_norm[\"y\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jOP6Q5qBQBn"
   },
   "source": [
    "LEAKAGE CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCl3-_oA-SAt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "608edc2f-1a67-4cf2-b22f-7d386b6c3939"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "X_meta = deg_text_norm[[\"word_count_norm\"]]\n",
    "y = deg_text_norm[\"y\"].astype(int).values\n",
    "groups = deg_text_norm[\"group_id\"].astype(str).values\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "\n",
    "clf = LogisticRegression(max_iter=5000)\n",
    "for tr, te in sgkf.split(X_meta, y, groups=groups):\n",
    "    clf.fit(X_meta.iloc[tr], y[tr])\n",
    "    oof[te] = clf.predict_proba(X_meta.iloc[te])[:,1]\n",
    "\n",
    "print(\"META-only (length only) ROC-AUC:\", roc_auc_score(y, oof))\n",
    "print(\"META-only (length only) PR-AUC :\", average_precision_score(y, oof))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zl4rXSPlBSty",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7862982f-9c7f-4765-c0c3-c0aff4ea5c11"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "X = deg_text_norm[\"doc_text_norm\"].fillna(\"\").astype(str).values\n",
    "y = deg_text_norm[\"y\"].astype(int).values\n",
    "groups = deg_text_norm[\"group_id\"].astype(str).values\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "y_shuf = rng.permutation(y)\n",
    "\n",
    "tfidf = FeatureUnion([\n",
    "    (\"word\", TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=2)),\n",
    "    (\"char\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=2)),\n",
    "])\n",
    "model = Pipeline([(\"tfidf\", tfidf), (\"lr\", LogisticRegression(max_iter=4000))])\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "for tr, te in sgkf.split(X, y_shuf, groups=groups):\n",
    "    model.fit(X[tr], y_shuf[tr])\n",
    "    oof[te] = model.predict_proba(X[te])[:,1]\n",
    "\n",
    "print(\"SHUFFLE ROC-AUC:\", roc_auc_score(y_shuf, oof))\n",
    "print(\"SHUFFLE PR-AUC :\", average_precision_score(y_shuf, oof))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7bVfYKYBdc_"
   },
   "source": [
    "TFIDF model on normalized text (grouped CV + threshold @5% FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeaqULd0BWP2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "175a1eca-f6f7-4a6b-b699-d29595360ac3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
    "\n",
    "def threshold_at_fpr(y_true, y_score, target_fpr=0.05):\n",
    "    y_true = np.asarray(y_true); y_score = np.asarray(y_score)\n",
    "    neg = y_score[y_true==0]\n",
    "    return float(np.quantile(neg, 1-target_fpr)) if len(neg) else 0.5\n",
    "\n",
    "X = deg_text_norm[\"doc_text_norm\"].fillna(\"\").astype(str).values\n",
    "y = deg_text_norm[\"y\"].astype(int).values\n",
    "groups = deg_text_norm[\"group_id\"].astype(str).values\n",
    "\n",
    "tfidf = FeatureUnion([\n",
    "    (\"word\", TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=2)),\n",
    "    (\"char\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=2)),\n",
    "])\n",
    "tfidf_lr_norm = Pipeline([(\"tfidf\", tfidf), (\"lr\", LogisticRegression(max_iter=4000))])\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "for tr, te in sgkf.split(X, y, groups=groups):\n",
    "    tfidf_lr_norm.fit(X[tr], y[tr])\n",
    "    oof[te] = tfidf_lr_norm.predict_proba(X[te])[:,1]\n",
    "\n",
    "print(\"TFIDF normalized ROC-AUC:\", roc_auc_score(y, oof))\n",
    "print(\"TFIDF normalized PR-AUC :\", average_precision_score(y, oof))\n",
    "\n",
    "thr_norm = threshold_at_fpr(y, oof, 0.05)\n",
    "pred = (oof >= thr_norm).astype(int)\n",
    "\n",
    "print(\"thr (~5% FPR):\", thr_norm)\n",
    "print(\"Confusion:\\n\", confusion_matrix(y, pred))\n",
    "print(classification_report(y, pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRErSRgoBgcr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8e6881a3-d630-44c7-dcc1-77650164a0ac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# choose the dataset you want to analyze:\n",
    "#   - df_degree_text        (original)\n",
    "#   - deg_text_norm         (proxy+OCR normalized)\n",
    "meta_df = deg_text_norm.copy()  # change if needed\n",
    "\n",
    "# build metadata table\n",
    "meta_df[\"file_type\"] = meta_df[\"file_type\"].astype(str)\n",
    "X_meta = meta_df[[\"word_count_norm\", \"file_type\"]]  # for normalized run\n",
    "y = meta_df[\"y\"].astype(int).values\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"file_type\"]),\n",
    "    (\"num\", \"passthrough\", [\"word_count_norm\"]),\n",
    "])\n",
    "\n",
    "meta_lr = Pipeline([(\"pre\", pre), (\"lr\", LogisticRegression(max_iter=5000))])\n",
    "meta_lr.fit(X_meta, y)\n",
    "\n",
    "ohe = meta_lr.named_steps[\"pre\"].named_transformers_[\"cat\"]\n",
    "feat_names = list(ohe.get_feature_names_out([\"file_type\"])) + [\"word_count_norm\"]\n",
    "\n",
    "coefs = meta_lr.named_steps[\"lr\"].coef_.ravel()\n",
    "imp = pd.DataFrame({\"feature\": feat_names, \"coef\": coefs, \"abs_coef\": np.abs(coefs)}).sort_values(\"abs_coef\", ascending=False)\n",
    "\n",
    "print(imp.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lw6_VENpDxGo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "25d7bdeb-6128-484d-fed4-da30f088bda8"
   },
   "outputs": [],
   "source": [
    "# Use your ablation dataset\n",
    "deg_text_norm = deg_text_norm.copy()\n",
    "deg_text_norm[\"file_type_const\"] = \"img\"  # constant for all\n",
    "\n",
    "# Sanity: should be only one value\n",
    "print(deg_text_norm[\"file_type_const\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpcY0LusENQ0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1abdd6b0-7641-4061-a1a7-81f011bc9cd7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "X_meta = deg_text_norm[[\"word_count_norm\"]]  # remove file_type entirely\n",
    "y = deg_text_norm[\"y\"].astype(int).values\n",
    "groups = deg_text_norm[\"group_id\"].astype(str).values\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "\n",
    "lr = LogisticRegression(max_iter=5000)\n",
    "for tr, te in sgkf.split(X_meta, y, groups=groups):\n",
    "    lr.fit(X_meta.iloc[tr], y[tr])\n",
    "    oof[te] = lr.predict_proba(X_meta.iloc[te])[:,1]\n",
    "\n",
    "print(\"META-only (length only; file_type removed) ROC-AUC:\", roc_auc_score(y, oof))\n",
    "print(\"META-only (length only; file_type removed) PR-AUC :\", average_precision_score(y, oof))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzrtTdduEvLn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ec076690-aff8-4a94-a5ec-3b167d2e1811"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tmp = deg_text_norm.copy()\n",
    "tmp[\"wc_bin\"] = pd.qcut(tmp[\"word_count_norm\"].clip(upper=4000), q=6, duplicates=\"drop\")\n",
    "\n",
    "parts = []\n",
    "for b, g in tmp.groupby(\"wc_bin\"):\n",
    "    n0 = (g[\"y\"]==0).sum()\n",
    "    n1 = (g[\"y\"]==1).sum()\n",
    "    n = min(n0, n1)\n",
    "    if n >= 3:\n",
    "        parts.append(g[g[\"y\"]==0].sample(n, random_state=42))\n",
    "        parts.append(g[g[\"y\"]==1].sample(n, random_state=42))\n",
    "\n",
    "deg_len_matched = pd.concat(parts).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"deg_len_matched:\", deg_len_matched.shape)\n",
    "print(deg_len_matched[\"y\"].value_counts())\n",
    "print(deg_len_matched.groupby(\"y\")[\"word_count_norm\"].describe()[[\"mean\",\"std\",\"min\",\"max\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9Doc1qEFBPi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bd11cd85-fe10-41bd-edf0-54860c89f53a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "X_meta = deg_len_matched[[\"word_count_norm\"]]\n",
    "y = deg_len_matched[\"y\"].astype(int).values\n",
    "groups = deg_len_matched[\"group_id\"].astype(str).values\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "\n",
    "lr = LogisticRegression(max_iter=5000)\n",
    "for tr, te in sgkf.split(X_meta, y, groups=groups):\n",
    "    lr.fit(X_meta.iloc[tr], y[tr])\n",
    "    oof[te] = lr.predict_proba(X_meta.iloc[te])[:,1]\n",
    "\n",
    "print(\"META-only (length-matched) ROC-AUC:\", roc_auc_score(y, oof))\n",
    "print(\"META-only (length-matched) PR-AUC :\", average_precision_score(y, oof))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDjLBXEZFQH1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9100cd7b-8868-43a2-f84e-6f3c1f0e2cfd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "X = deg_len_matched[\"doc_text_norm\"].fillna(\"\").astype(str).values\n",
    "y = deg_len_matched[\"y\"].astype(int).values\n",
    "groups = deg_len_matched[\"group_id\"].astype(str).values\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "y_shuf = rng.permutation(y)\n",
    "\n",
    "tfidf = FeatureUnion([\n",
    "    (\"word\", TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=2)),\n",
    "    (\"char\", TfidfVectorizer(analyzer=\"char_wb\", max_features=40000, ngram_range=(3,5), min_df=2)),\n",
    "])\n",
    "model = Pipeline([(\"tfidf\", tfidf), (\"lr\", LogisticRegression(max_iter=4000))])\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "for tr, te in sgkf.split(X, y_shuf, groups=groups):\n",
    "    model.fit(X[tr], y_shuf[tr])\n",
    "    oof[te] = model.predict_proba(X[te])[:,1]\n",
    "\n",
    "print(\"SHUFFLE (len-matched) ROC-AUC:\", roc_auc_score(y_shuf, oof))\n",
    "print(\"SHUFFLE (len-matched) PR-AUC :\", average_precision_score(y_shuf, oof))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDAQYNoAFVm7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bda2948b-6460-48da-bea8-541155ba3c55"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "K_WORDS = 200\n",
    "PAD = \"zzpadzz\"  # will be ignored by vectorizer via stop_words\n",
    "\n",
    "def fixed_len_text(t: str, k=200, pad=PAD) -> str:\n",
    "    t = (t or \"\").strip().lower()\n",
    "    toks = re.findall(r\"[a-zA-Z]+|<num>\", t)  # keep words + <num> if you already use it\n",
    "    if len(toks) >= k:\n",
    "        toks = toks[:k]\n",
    "    else:\n",
    "        toks = toks + [pad] * (k - len(toks))\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# Use the ablation dataset with proxy OCR:\n",
    "# deg_text_norm (you already have)\n",
    "deg_text_norm[\"doc_text_fixed\"] = deg_text_norm[\"doc_text_norm\"].fillna(\"\").astype(str).apply(lambda x: fixed_len_text(x, K_WORDS, PAD))\n",
    "\n",
    "print(\"doc_text_fixed created. Example word_count_fixed:\",\n",
    "      deg_text_norm[\"doc_text_fixed\"].str.split().str.len().value_counts().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5p74OcF0GZSE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "40d09667-8069-413b-a3a0-98dcb19aeb46"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "deg_text_norm[\"word_count_fixed\"] = deg_text_norm[\"doc_text_fixed\"].str.split().str.len().astype(int)\n",
    "\n",
    "print(deg_text_norm[\"word_count_fixed\"].describe())\n",
    "print(deg_text_norm[\"word_count_fixed\"].value_counts().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDkt2ru-GqYO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e1da45f0-29f5-497b-ed90-6421023e23fc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "X_meta = deg_text_norm[[\"word_count_fixed\"]]  # should be constant => near random\n",
    "y = deg_text_norm[\"y\"].astype(int).values\n",
    "groups = deg_text_norm[\"group_id\"].astype(str).values\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "\n",
    "lr = LogisticRegression(max_iter=5000)\n",
    "for tr, te in sgkf.split(X_meta, y, groups=groups):\n",
    "    lr.fit(X_meta.iloc[tr], y[tr])\n",
    "    oof[te] = lr.predict_proba(X_meta.iloc[te])[:,1]\n",
    "\n",
    "print(\"META-only (word_count_fixed) ROC-AUC:\", roc_auc_score(y, oof))\n",
    "print(\"META-only (word_count_fixed) PR-AUC :\", average_precision_score(y, oof))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dw_5Mr1QGvbC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "01adb788-4146-4209-ba7e-860eb864be79"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "X = deg_text_norm[\"doc_text_fixed\"].fillna(\"\").astype(str).values\n",
    "y = deg_text_norm[\"y\"].astype(int).values\n",
    "groups = deg_text_norm[\"group_id\"].astype(str).values\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "y_shuf = rng.permutation(y)\n",
    "\n",
    "# Binary TF + ignore PAD token => reduces length/frequency artifacts\n",
    "vec = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=1,\n",
    "    binary=True,\n",
    "    stop_words=[PAD],\n",
    "    norm=\"l2\"\n",
    ")\n",
    "\n",
    "model = Pipeline([(\"tfidf\", vec), (\"lr\", LogisticRegression(max_iter=4000))])\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "\n",
    "for tr, te in sgkf.split(X, y_shuf, groups=groups):\n",
    "    model.fit(X[tr], y_shuf[tr])\n",
    "    oof[te] = model.predict_proba(X[te])[:,1]\n",
    "\n",
    "print(\"SHUFFLE (fixed-length full) ROC-AUC:\", roc_auc_score(y_shuf, oof))\n",
    "print(\"SHUFFLE (fixed-length full) PR-AUC :\", average_precision_score(y_shuf, oof))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKGxQTfMHS-K"
   },
   "source": [
    "TFIDF + LR (word+char) on doc_text_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gfs0i4ZhG4y1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0c94c0bd-602d-4589-8289-5dda85f64d6f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
    "\n",
    "X = deg_text_norm[\"doc_text_fixed\"].fillna(\"\").astype(str).values\n",
    "y = deg_text_norm[\"y\"].astype(int).values\n",
    "groups = deg_text_norm[\"group_id\"].astype(str).values\n",
    "\n",
    "tfidf = FeatureUnion([\n",
    "    (\"word\", TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=2,\n",
    "                             binary=True, stop_words=[PAD])),\n",
    "    (\"char\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=2,\n",
    "                             binary=True)),\n",
    "])\n",
    "\n",
    "clf = Pipeline([(\"tfidf\", tfidf), (\"lr\", LogisticRegression(max_iter=4000))])\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "\n",
    "for tr, te in sgkf.split(X, y, groups=groups):\n",
    "    clf.fit(X[tr], y[tr])\n",
    "    oof[te] = clf.predict_proba(X[te])[:,1]\n",
    "\n",
    "print(\"TFIDF (fixed-length) ROC-AUC:\", roc_auc_score(y, oof))\n",
    "print(\"TFIDF (fixed-length) PR-AUC :\", average_precision_score(y, oof))\n",
    "\n",
    "# threshold at ~5% FPR\n",
    "thr = float(np.quantile(oof[y==0], 0.95))\n",
    "pred = (oof >= thr).astype(int)\n",
    "\n",
    "print(\"thr (~5% FPR):\", thr)\n",
    "print(\"Confusion:\\n\", confusion_matrix(y, pred))\n",
    "print(classification_report(y, pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8_WRhbOHz9K"
   },
   "source": [
    "PLM Embeddings + LR on doc_text_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wn7mRkaLHSMl",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "3863ba32fe5440a695515d52ffa6a372",
      "f375a6a8ddb9415db5536f7f05a6ebb7",
      "d775fc38aa404aac9963a6aef1b536a2",
      "a83b5192e5d74eb2bdb30edccda263bb",
      "3347d672499e4c8dadfe95427db2e428",
      "ad233f3c42dd4054aba1508337d833ef",
      "927bcea105a744589b5d03e30d5ac1a3",
      "3bff31e2f2b040a295e971ba11fbac1c",
      "7a20e36b50fc48c6bbc1eb44971f9bbf",
      "9a8ec223150943b4adc1c8ba9defac04",
      "0a497dfa642049c4affa5e39c3452260"
     ]
    },
    "outputId": "4497f031-19e4-47db-d7ab-36f4e127ba73"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U sentence-transformers\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "emb = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "texts = deg_text_norm[\"doc_text_fixed\"].fillna(\"\").astype(str).tolist()\n",
    "y = deg_text_norm[\"y\"].astype(int).values\n",
    "groups = deg_text_norm[\"group_id\"].astype(str).values\n",
    "\n",
    "X_emb = emb.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "clf = LogisticRegression(max_iter=5000)\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "for tr, te in sgkf.split(X_emb, y, groups=groups):\n",
    "    clf.fit(X_emb[tr], y[tr])\n",
    "    oof[te] = clf.predict_proba(X_emb[te])[:,1]\n",
    "\n",
    "print(\"EMB (fixed-length) ROC-AUC:\", roc_auc_score(y, oof))\n",
    "print(\"EMB (fixed-length) PR-AUC :\", average_precision_score(y, oof))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHaO_PHCKSFX"
   },
   "source": [
    "RoBERTa fine-tune (group holdout) on doc_text_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzCCsqCeH4ND",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787,
     "referenced_widgets": [
      "5e11516f4e954dd693ad1613e2baeade",
      "22a04159cee64df4966e42d9e8df2430",
      "758f5e782e504950bae80725ed8e789b",
      "84aa30f52bb647908e9486091f313ef1",
      "8515e7ca685d4161a3eab92e80b27e0f",
      "77c16ade433b44b79239500f17dfcf02",
      "f6cdeb51f7d245bbaedd18f9870a1eaa",
      "3b3470ae04614a5face4a803fea6b9fc",
      "2abdb37fe55448b6a4e2a78b37d49aae",
      "5d61c8bd30424ae2a24ff0e6183bcc6e",
      "5e6e20deeeea42628570ad31e0da19cc",
      "7fa282ef8bd14651b6008a4ccdf4def8",
      "3c19cec3c2fa44f08190e28455228cf9",
      "93a96ffdb43945399498817a5a844d36",
      "fdf01b12b5cc409ab165574d6b387208",
      "705facdee19d4d6baf0ea3ce3ddc93e2",
      "4c2409ec90944e1a88e11b2cf5bbec0e",
      "0da2a71a44794296bffb967fff19ec01",
      "969cbff7062f478bbabdcdd53814fa7b",
      "010fbb28aca54c5aa4d888b00ea71f65",
      "04662a9a3ccc42bdbe063d8f6ff92975",
      "313afd374530410aab7eb654c2646a61",
      "cd2572e215a54bb5bb0bea104dff6dd6",
      "ec12bda0620c4fc29ef9f9b36ecf2b83",
      "437f76f2a5bc48639bc3132d27033090",
      "f3805ec8538d46db82ef9ebfb47034a2",
      "b43b737578cd4d39a09461038c3eb9e6",
      "5d0f54c338394d32bf2dfa6adb78dbbd",
      "246182551dd04c35968a11eb5373d214",
      "d35ca60a09bf46998892ef7f6edbb41a",
      "e1750ecab16b4f658f2e946cd077903f",
      "089c6bb0e856452cb5abfc194ca36ddd",
      "4b2cbc64338746c9910e13d0b8e29660",
      "e0003c359720414cae63f5d9a4261898",
      "65c9a93223964ea6b0ffc2c0222f2cd8",
      "5eff1823c8a64cdba48bb7e2a6370cee",
      "1e55e34812ca4c3092b2bd1f88d8594f",
      "f6a1d23763f949268bf8dd3bc0e465ad",
      "4d559cb71c6c41f182bcf8ba279188f3",
      "4804b5a86bf24f5697615bd9b6ae5831",
      "7143428ceb744833966a2cc5caccb9ba",
      "c1fa944c09b244e4bc43eb5cb89d7fe3",
      "a690818693684c578a736e5a098893a5",
      "f3be1a032825464dab6305b872095012",
      "2ed52cab26d14c6b9e04d953de590e9e",
      "3bddef17a20d4ebaa145fa4f8a3bb30c",
      "8ac3aa30932c40d29c145cfa7ab5f5e0",
      "cb35ef95e4d94a658b44a7054444f112",
      "b77eab37a1c74ffba27286c7e223302e",
      "b17538f97a9341d8921c9925e5c09b88",
      "09dd9f73f99142c29072b87063d2157d",
      "cfec8451b55045048b3627967023e944",
      "f1d8232e7b204d5eae0cc4ba38c96bcd",
      "994b3313827442f18d8aff31aff6ee77",
      "4897278d2ec74065921e74c64662b668",
      "aad5bde27c5a48a1b32273c573a8da2b",
      "ed6307fbb98c4ef5918dd4e337a8d29e",
      "4587321c9aa04fdcacd9b3dcbc38d632",
      "58cdce2180414997a11a5fbf6a17ae6b",
      "dd967c5358e24d7f8abb3fd2a924dd69",
      "cbd7a1db16784136b99804200abf6ad1",
      "30cbf842d69f406b9d24a19aa09e604e",
      "21d047f215174357830435bcb5e04025",
      "f779c6af01ac460080e3074004c7d2d9",
      "66b6fcb9bd8448fbad0bdf596162ef6b",
      "2ab70982e27e48bca0047ac7513ca87b",
      "37eb870ecde74983a26ad7f9f2c8329c",
      "014e867225f84dac941b2ac5992d9717",
      "0fd23ca7e6444a0fb4d41bedd3689646",
      "f288d53bf526485485d578781e65491b",
      "eb63c501212d4401ba21f66d248a8d24",
      "19bf247b1a694237ab1162092f01faeb",
      "cfafb988760a4b3193386db812191e1c",
      "f690d3a09c244ed98747b0437625c0c3",
      "04a8a333003644fd9ec284da44ab7764",
      "3de1196a1a834ac590853879381ab7d5",
      "4aab38e6d3c443ecb7bfd447973b577c",
      "cf1e8d7671fb4f818eb508837eb98f61",
      "fe02ac91b26046f180048ba268d7e72a",
      "1f305aa15b7d45208fa65147a19f97f5",
      "6e0e3f86fba24b1dbcb597b589fd706c",
      "5daef50b9a23448dad124ecdf58aaba8",
      "00864212992e47709887b07bc122f409",
      "4c78f396842a4cb880e1c6703f428b58",
      "f0afdcf8a9d44a319ea703aee94c768c",
      "2233f2d7dbf34c6481a6ad40972dd360",
      "c7f9d9398f2440f0b55874c076ecf36b",
      "a47e6d7e114648ea9e7d9b140828b8f4"
     ]
    },
    "outputId": "9adf991f-8eab-457d-fa7e-72ef0e1fe6b9"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U transformers datasets accelerate\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "model_ckpt = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "idx = np.arange(len(deg_text_norm))\n",
    "y = deg_text_norm[\"y\"].astype(int).values\n",
    "groups = deg_text_norm[\"group_id\"].astype(str).values\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(idx, y, groups=groups))\n",
    "\n",
    "train_df = deg_text_norm.iloc[train_idx][[\"doc_text_fixed\",\"y\"]].rename(columns={\"doc_text_fixed\":\"doc_text\",\"y\":\"label\"})\n",
    "test_df  = deg_text_norm.iloc[test_idx][[\"doc_text_fixed\",\"y\"]].rename(columns={\"doc_text_fixed\":\"doc_text\",\"y\":\"label\"})\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "test_ds  = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"doc_text\"], truncation=True, max_length=512)\n",
    "\n",
    "train_ds = train_ds.map(tok, batched=True).remove_columns([\"doc_text\"])\n",
    "test_ds  = test_ds.map(tok, batched=True).remove_columns([\"doc_text\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ft_out_fixed\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=test_ds, tokenizer=tokenizer)\n",
    "trainer.train()\n",
    "\n",
    "pred = trainer.predict(test_ds).predictions\n",
    "proba = np.exp(pred)[:,1] / np.exp(pred).sum(axis=1)\n",
    "y_test = test_df[\"label\"].astype(int).values\n",
    "\n",
    "print(\"RoBERTa (fixed-length) ROC-AUC:\", roc_auc_score(y_test, proba))\n",
    "print(\"RoBERTa (fixed-length) PR-AUC :\", average_precision_score(y_test, proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IE7Iu7RKWjs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4eb85f1d-4d21-46e0-9714-76c82dff6e59"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X_all = deg_text_norm[\"doc_text_fixed\"].fillna(\"\").astype(str).values\n",
    "y_all = deg_text_norm[\"y\"].astype(int).values\n",
    "groups_all = deg_text_norm[\"group_id\"].astype(str).values\n",
    "idx = np.arange(len(deg_text_norm))\n",
    "\n",
    "def find_good_group_split(test_size=0.2, max_tries=200, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    for t in range(max_tries):\n",
    "        rs = int(rng.randint(0, 10_000_000))\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=rs)\n",
    "        tr, te = next(gss.split(idx, y_all, groups=groups_all))\n",
    "        y_te = y_all[te]\n",
    "        if len(np.unique(y_te)) == 2:\n",
    "            return tr, te, rs\n",
    "    raise RuntimeError(\"Could not find a group split with both classes in test. Reduce test_size or change group_id strategy.\")\n",
    "\n",
    "train_idx, test_idx, used_seed = find_good_group_split(test_size=0.2, seed=42)\n",
    "print(\"✅ Found split seed:\", used_seed)\n",
    "print(\"Train size:\", len(train_idx), \"Test size:\", len(test_idx))\n",
    "print(\"Test y counts:\", dict(zip(*np.unique(y_all[test_idx], return_counts=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "model_ckpt = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "train_df = deg_text_norm.iloc[train_idx][[\"doc_text_fixed\",\"y\"]].rename(columns={\"doc_text_fixed\":\"doc_text\",\"y\":\"label\"})\n",
    "test_df  = deg_text_norm.iloc[test_idx][[\"doc_text_fixed\",\"y\"]].rename(columns={\"doc_text_fixed\":\"doc_text\",\"y\":\"label\"})\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "test_ds  = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"doc_text\"], truncation=True, max_length=256)\n",
    "\n",
    "train_ds = train_ds.map(tok, batched=True).remove_columns([\"doc_text\"])\n",
    "test_ds  = test_ds.map(tok, batched=True).remove_columns([\"doc_text\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ft_out_fixed_quick\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,        # lighter than 2\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",          # avoid wandb prompt\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=test_ds, tokenizer=tokenizer)\n",
    "trainer.train()\n",
    "\n",
    "pred = trainer.predict(test_ds).predictions\n",
    "proba = np.exp(pred)[:,1] / np.exp(pred).sum(axis=1)\n",
    "y_test = test_df[\"label\"].astype(int).values\n",
    "\n",
    "print(\"RoBERTa (fixed-length) ROC-AUC:\", roc_auc_score(y_test, proba))\n",
    "print(\"RoBERTa (fixed-length) PR-AUC :\", average_precision_score(y_test, proba))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292,
     "referenced_widgets": [
      "9071506d24134f61bc3c84ef037ec999",
      "a723eb45c5254884a0dfddb877c03fd9",
      "afcdafa2717c49c28071ce8dd1e839bd",
      "dd9a7650c6504dfba88b68f931542dc2",
      "7d5737a1281e4b308b7aa95d1d1aecfc",
      "218bf086293b4fae80892c911b0ca186",
      "9fc3a2fcf01042a78fe34ea57d5707f8",
      "7defd643a24744818a813383afc84ffa",
      "463ee1cfe5e74f80a66dc55c18d23f6b",
      "74836d15ced24012bc8e95ca1679c34b",
      "ad8fe392addf493db0534f27b40245a1",
      "99c84c8501b24d5581dc59ad9ccd50c5",
      "ca221724f98442b1bd8b5f06c32e22d2",
      "c802add5aa8b409a969fa445456ff67b",
      "53af947fe7a44d91b986b47b97056212",
      "562b9130da7847f689113f5844f1fe14",
      "c1c1a6966bf84be4b15e378d297ed667",
      "7e6a7c9a5afc4a809fd73429af684ef8",
      "50782fd4597b4dc5a59c8f2d172c2b2e",
      "b7ed3a4677cc4492bfa1a5bb582c010d",
      "b7ace15f4d2340e1889b75bc8e01646e",
      "44ab4204a12d474bb779afa3f83dbaa5"
     ]
    },
    "id": "BR-s900BQujA",
    "outputId": "63592bf6-9bbf-4175-86c7-09a039ed7641"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json, os\n",
    "from pathlib import Path\n",
    "\n",
    "outdir = Path(\"artifacts_roberta_fixed\")\n",
    "outdir.mkdir(exist_ok=True)\n",
    "\n",
    "# save model + tokenizer\n",
    "trainer.save_model(str(outdir))\n",
    "tokenizer.save_pretrained(str(outdir))\n",
    "\n",
    "# save metadata\n",
    "meta = {\n",
    "    \"model_ckpt\": \"roberta-base\",\n",
    "    \"text_col\": \"doc_text_fixed\",\n",
    "    \"label_col\": \"y\",\n",
    "    \"group_col\": \"group_id\",\n",
    "    \"train_size\": int(len(train_idx)),\n",
    "    \"test_size\": int(len(test_idx)),\n",
    "    \"roc_auc\": float(0.8997668997668997),\n",
    "    \"pr_auc\": float(0.9510900050081298),\n",
    "    \"split_seed\": int(used_seed),\n",
    "}\n",
    "with open(outdir/\"meta.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"Saved to:\", outdir)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLOJNLrLQx6f",
    "outputId": "fd6531c5-9f43-4db0-eaeb-abb8ee84978f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "X = deg_text_norm[\"doc_text_fixed\"].fillna(\"\").astype(str).values\n",
    "y = deg_text_norm[\"y\"].astype(int).values\n",
    "groups = deg_text_norm[\"group_id\"].astype(str).values\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "\n",
    "for tr, te in sgkf.split(X, y, groups=groups):\n",
    "    tfidf_lr.fit(X[tr], y[tr])\n",
    "    oof[te] = tfidf_lr.predict_proba(X[te])[:,1]\n",
    "\n",
    "print(\"TFIDF (fixed-length) ROC-AUC:\", roc_auc_score(y, oof))\n",
    "print(\"TFIDF (fixed-length) PR-AUC :\", average_precision_score(y, oof))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqjH-JeTRKUN",
    "outputId": "81cf3642-bc63-4f8c-e77f-c68c86949837"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip -q install -U sentence-transformers\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "emb = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "texts = deg_text_norm[\"doc_text_fixed\"].fillna(\"\").astype(str).tolist()\n",
    "y = deg_text_norm[\"y\"].astype(int).values\n",
    "groups = deg_text_norm[\"group_id\"].astype(str).values\n",
    "\n",
    "X_emb = emb.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "clf = LogisticRegression(max_iter=5000)\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(y), dtype=float)\n",
    "for tr, te in sgkf.split(X_emb, y, groups=groups):\n",
    "    clf.fit(X_emb[tr], y[tr])\n",
    "    oof[te] = clf.predict_proba(X_emb[te])[:,1]\n",
    "\n",
    "print(\"MiniLM-EMB (fixed-length) ROC-AUC:\", roc_auc_score(y, oof))\n",
    "print(\"MiniLM-EMB (fixed-length) PR-AUC :\", average_precision_score(y, oof))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "e2b235d6cc0c41dfa2b9490a67504ac6",
      "cc5d0cf3dd144397bd97fc7d680fc9b6",
      "67f09b60783e48889ab0d704599a7412",
      "13fe5059b2e84eb58f4841ebe18167a5",
      "89bcaafb366847998ed63b5f354bec8b",
      "53c7a62a5499452dacdc92dc28320b25",
      "392a3f1d331a49d1b576d435c93a558f",
      "c649d6e417a74fecb222537646329c8c",
      "e230feb52c8148b0842fff85ff102ef7",
      "7f0ebe366a8b472ab3840460e0bd30a5",
      "3689b4fc8b744a0ab9a80057b73037e2"
     ]
    },
    "id": "GEy8EufzRUtN",
    "outputId": "79dd66db-b6f8-4cf0-e58c-18fcf9055abf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "need_cols = [\"doc_text_fixed\",\"proxy_img_path\",\"y\",\"group_id\"]\n",
    "missing = [c for c in need_cols if c not in deg_text_norm.columns]\n",
    "assert not missing, f\"Missing columns in deg_text_norm: {missing}\"\n",
    "\n",
    "# drop rows with missing proxy images (should be none if you created proxies)\n",
    "deg_mm = deg_text_norm[deg_text_norm[\"proxy_img_path\"].fillna(\"\") != \"\"].copy()\n",
    "print(\"deg_mm:\", deg_mm.shape, \"| y:\", deg_mm[\"y\"].value_counts().to_dict())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfHI71TCRi79",
    "outputId": "c9d29aee-6362-4b7e-e092-5edac9fda09d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract CLIP image embeddings for proxy_img_path"
   ],
   "metadata": {
    "id": "RETWJkQeTCYS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip -q install -U transformers pillow\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(clip_name).to(device)\n",
    "clip_proc  = CLIPProcessor.from_pretrained(clip_name)\n",
    "\n",
    "paths = deg_mm[\"proxy_img_path\"].astype(str).tolist()\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_encode_images(paths, batch_size=32):\n",
    "    embs = []\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        batch_paths = paths[i:i+batch_size]\n",
    "        imgs = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "        inputs = clip_proc(images=imgs, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        feats = clip_model.get_image_features(**inputs)\n",
    "        feats = torch.nn.functional.normalize(feats, p=2, dim=1)\n",
    "        embs.append(feats.cpu().numpy())\n",
    "    return np.vstack(embs)\n",
    "\n",
    "X_img = clip_encode_images(paths, batch_size=32)\n",
    "print(\"X_img:\", X_img.shape)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410,
     "referenced_widgets": [
      "159d191b19b24c03b8620710291d7643",
      "9dcdeaf3ac254da48ea4bdac05760e6f",
      "0b13cd692d764b93bf5b74fb438e795c",
      "6dd8fdf4ef6945dda32a212ee205019f",
      "57788eac24a74efc9a0070a06b8804a5",
      "0ac3033103d34506bb36f4753ded0080",
      "f8f6c24a723d48ba85be2648660a18dd",
      "556cded830ac450896864f60729f9429",
      "46c4633554c84195af12488ba31a60b6",
      "8493a264925d4d6b82ce955ffa305d31",
      "09fc5ca54fc3431696d4fc29c871b9f6",
      "ef2c5a908c914318ab0b37236f25d5a6",
      "8fd37841be3441be9cfd676315bc35dc",
      "a6a58c5e7d78408aacecc06be81eee7e",
      "ea25269c0cf54815ae58fe2d53c0a887",
      "1e3ecd7fd771475eb6ff62ced087f783",
      "dc4ae86760184075b94ef0251e0a5433",
      "7e48309dbaa1475e8b44dddd1f3063ff",
      "42cc9b36d4914e7e91a084eb25334164",
      "db196c71e5ec4d94a550c0bb356fac05",
      "7c8d3a094b55458792d84273df61b649",
      "37d0ccddb82746dea60842268e1899bc",
      "a5eea9e741b840f1a1e5cd98c96af037",
      "5124224c590f48c8947f062ae36fcbf8",
      "b7c22d23a1514ba29918f5b307c7b485",
      "af02d378a3944d7b9d93100381b9309c",
      "f6deddfeb2b54872af07a8bb8ea99a1b",
      "1ccafdda9ff2475f8360de9b0b7723f6",
      "011ea9743e444f20a7cf0ec1a0f3e93a",
      "0882ee600d3d4a0baa0c94e172294c4c",
      "be10f97fa2494eed8dcd2264e56ce78f",
      "1f59b17a9f4141669e7ed5d325f4ad4a",
      "78f9348827a04dacad666f4a13b7c9b0",
      "0d1d2528143147b4aaf2ef55cf79c0da",
      "288205110a1e4e4da86044c0c1fb0dbe",
      "f8166b78cd5645a698716b37a42a855c",
      "0c99bbf7fe304c59a0f1b48497c2624e",
      "a19d67df5fbd4e2bb4831e2a10a9e194",
      "ff8b8b5cdd8a4ba686fd4da61cd8bf98",
      "ba3d05c7e2ad4e23997503ace7b07547",
      "f2b4c9cd65d045b6b8f0f24dab533f8a",
      "7b5e476a5a5340828f13668cfef80d18",
      "eb141ee728b54704af3aab926efaf54a",
      "1bfb11cb09c546b08daaaa60e07447eb",
      "ff7491c84670451f9f3d3ee32cf201d3",
      "a8e9a2f95cd341e28aae91664a61d2b6",
      "4c08a4fea4a5452f8ed09aa1025f4610",
      "2b7c6c97f13b4440a559d070ae3549eb",
      "1faf6d7fb47242c9a20ea8d3d6c5ff5f",
      "b52520cbdefc43f59a2f8663a3ac4b2e",
      "682a5eb0f40a4d28965777fb9961d1ba",
      "55ee7c61879c4e06a622a8a94d6470cb",
      "cb65822d74ac432c915f0829aa91c756",
      "fa1d47ac20434a329e2cb54b19524108",
      "41726894bef049038a43a6de11649cd5",
      "ff1f4f26d33645068b4674ad75dad942",
      "1bcd71cb22884b9face363316311ca23",
      "1c9c3a0e00c84fb1a8d76eb455ba4da4",
      "5994cd16a1444755ad2769838610e819",
      "d8d97c6b0064485e88035c2b25640719",
      "848b5e6c000e4af388250b24ba076548",
      "e450d597951b4734a3bbb8d46332dc6f",
      "6a3414ecba0f408e89c9cefa212ba57f",
      "f4cc388ea7214bfcaf7071aee21eae1e",
      "f6d6c39b843b43bd9acf7a9a41ae71bb",
      "4cd4387c87a44998b83c8228c928cb73",
      "901907ba6e1f4b99a81ea1d8b00a504e",
      "7d2fbb01173e4675aeca0647ebb6c6eb",
      "f252e5aded514975ab8eb8651d4a1d6a",
      "e903f35b445e4d8c9607d07b449e81dc",
      "846760d4cd0349b5a5c72276c6eb8b0e",
      "57684179dfd84276a6a42a716013b932",
      "c6e22669059249c9b88c85b4a10f2c73",
      "32bb17970f2f4560bb271779670e9352",
      "50308a53a19d40ed99b25c3ad3bcb0f1",
      "4a0d8d0aa0c94bb6b49bfda482a95d53",
      "c61d03ed90d94ca4a13b9dd3aee1e3b5",
      "1417818044a64949bc1954d564c2d9af",
      "4155fa05078b492ea18cbe10ee72d20c",
      "31fa73005da04795bbb80538eb654ffc",
      "6fdf47133f1542709eb04d20a8cd2949",
      "0fecff3feef7494681232c7abc78eea7",
      "58329e4d26d8438a9e97206346338f9e",
      "93d054bedeb74987906362cc7bf50c70",
      "0d65091f24184a7db2bfec17438d98b6",
      "67b28bf200cb41269af37d148132b3ee",
      "9f4f1043b7874e4490feb173a3042ff4",
      "8fa585178ef748659b7b6bc208c58455",
      "df218a935d9a44b19e27a4b737ac3835",
      "a621e9b7f4864cac9fff47d98584db33",
      "e69d5b4481144214a382c36b4d6e5a17",
      "ed2a2adf3d0041c28a388f9751c076bd",
      "8d2528bce25945b2bd652f88d9587722",
      "0181236a5d5c400e86838276a2c9bf08",
      "eb05200930804002bee57f514717a4ca",
      "9bced256a02e4f5b9cb6d20900fffb19",
      "575c27a9e61942a7a3416ba20126fdf6",
      "59baad27a1b8496e95309706dfef9f88",
      "d8b9787ab4794c718913ac73ee9498b4"
     ]
    },
    "id": "RPq0OVWeSb3A",
    "outputId": "d8e67918-9387-450d-9e9e-2fae95508e9f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "mage-only model (LR) with StratifiedGroupKFold OOF"
   ],
   "metadata": {
    "id": "d2N5GQPHTR2o"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "y = deg_mm[\"y\"].astype(int).values\n",
    "groups = deg_mm[\"group_id\"].astype(str).values\n",
    "\n",
    "img_lr = LogisticRegression(max_iter=5000)\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_img = np.zeros(len(y), dtype=float)\n",
    "for tr, te in sgkf.split(X_img, y, groups=groups):\n",
    "    img_lr.fit(X_img[tr], y[tr])\n",
    "    oof_img[te] = img_lr.predict_proba(X_img[te])[:,1]\n",
    "\n",
    "print(\"IMG-only ROC-AUC:\", roc_auc_score(y, oof_img))\n",
    "print(\"IMG-only PR-AUC :\", average_precision_score(y, oof_img))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1hS1jEbTMKk",
    "outputId": "7475a19c-dc79-4756-ea8c-28ef0b62f5c5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text-only TFIDF OOF on the SAME deg_mm order"
   ],
   "metadata": {
    "id": "m6gPpC3cT1_P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# If you ever re-ran and lost it, run:\n",
    "# PAD = \"zzpadzz\"\n",
    "# (and your tfidf_lr definition cell)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "X_txt = deg_mm[\"doc_text_fixed\"].fillna(\"\").astype(str).values\n",
    "\n",
    "oof_txt = np.zeros(len(y), dtype=float)\n",
    "for tr, te in sgkf.split(X_txt, y, groups=groups):\n",
    "    tfidf_lr.fit(X_txt[tr], y[tr])\n",
    "    oof_txt[te] = tfidf_lr.predict_proba(X_txt[te])[:,1]\n",
    "\n",
    "print(\"TFIDF-only ROC-AUC:\", roc_auc_score(y, oof_txt))\n",
    "print(\"TFIDF-only PR-AUC :\", average_precision_score(y, oof_txt))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1rxH-zuT2v2",
    "outputId": "4b2ff3ca-8fe3-4c7d-f739-1c77ca048613"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Late fusion (no extra training): best weighted average"
   ],
   "metadata": {
    "id": "MJDlIrkWUTOj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Late fusion (no extra training): best weighted average"
   ],
   "metadata": {
    "id": "OC5gnj6KT7Ya"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def eval_fusion(alpha):\n",
    "    fused = alpha*oof_txt + (1-alpha)*oof_img\n",
    "    return (\n",
    "        roc_auc_score(y, fused),\n",
    "        average_precision_score(y, fused),\n",
    "        fused\n",
    "    )\n",
    "\n",
    "alphas = np.linspace(0, 1, 21)  # 0.0..1.0 step 0.05\n",
    "best = None\n",
    "for a in alphas:\n",
    "    ra, pr, _ = eval_fusion(a)\n",
    "    cand = (pr, ra, a)\n",
    "    if best is None or cand > best:\n",
    "        best = cand\n",
    "\n",
    "best_pr, best_roc, best_a = best\n",
    "_, _, oof_fused = eval_fusion(best_a)\n",
    "\n",
    "print(\"Best alpha (txt weight):\", best_a)\n",
    "print(\"FUSED ROC-AUC:\", best_roc)\n",
    "print(\"FUSED PR-AUC :\", best_pr)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOG6yC93Ulqz",
    "outputId": "3daaf4ac-6561-4b45-e599-0b227d36e797"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Threshold @ ~5% FPR + confusion (using fused OOF)"
   ],
   "metadata": {
    "id": "_Jo2tvSqVZ5A"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "thr = float(np.quantile(oof_fused[y==0], 0.95))  # ~5% FPR target\n",
    "pred = (oof_fused >= thr).astype(int)\n",
    "\n",
    "print(\"thr (~5% FPR):\", thr)\n",
    "print(\"Confusion:\\n\", confusion_matrix(y, pred))\n",
    "print(classification_report(y, pred, digits=3))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7xuNRNlUrNY",
    "outputId": "dd868adb-6492-4a42-8b82-17e08d324728"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train final models on full data + save artifact"
   ],
   "metadata": {
    "id": "ADGzC_OMVhGO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import joblib, json\n",
    "from pathlib import Path\n",
    "\n",
    "# Train final text model\n",
    "tfidf_lr.fit(X_txt, y)\n",
    "\n",
    "# Train final image model\n",
    "img_lr.fit(X_img, y)\n",
    "\n",
    "outdir = Path(\"artifacts_multimodal_fusion\")\n",
    "outdir.mkdir(exist_ok=True)\n",
    "\n",
    "joblib.dump(tfidf_lr, outdir/\"tfidf_lr_fixed.joblib\")\n",
    "joblib.dump(img_lr,  outdir/\"clip_img_lr.joblib\")\n",
    "\n",
    "meta = {\n",
    "    \"dataset_rows\": int(len(deg_mm)),\n",
    "    \"text_col\": \"doc_text_fixed\",\n",
    "    \"img_col\": \"proxy_img_path\",\n",
    "    \"label_col\": \"y\",\n",
    "    \"group_col\": \"group_id\",\n",
    "    \"clip_model\": clip_name,\n",
    "    \"fusion\": {\"type\": \"weighted_average\", \"alpha_text\": float(best_a), \"thr_fpr05\": float(thr)},\n",
    "}\n",
    "\n",
    "with open(outdir/\"fusion_meta.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"Saved to:\", outdir)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aB9GIwfPVcjt",
    "outputId": "99d8b220-d395-4142-b5b2-5034a013e81a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "print(\"IMG-only ROC-AUC:\", roc_auc_score(y, oof_img))\n",
    "print(\"IMG-only PR-AUC :\", average_precision_score(y, oof_img))\n",
    "\n",
    "print(\"FUSED ROC-AUC:\", roc_auc_score(y, oof_fused))\n",
    "print(\"FUSED PR-AUC :\", average_precision_score(y, oof_fused))\n",
    "print(\"Best alpha (txt weight):\", best_a)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W326KZ7IVpyU",
    "outputId": "3a2758c6-29ba-40d9-dbbe-1e56e1e6d477"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import joblib, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "outdir = Path(\"artifacts_multimodal_fusion\")\n",
    "outdir.mkdir(exist_ok=True)\n",
    "\n",
    "# Train final models on full data\n",
    "tfidf_lr.fit(X_txt, y)\n",
    "img_lr.fit(X_img, y)\n",
    "\n",
    "joblib.dump(tfidf_lr, outdir/\"tfidf_lr_fixed.joblib\")\n",
    "joblib.dump(img_lr,  outdir/\"clip_img_lr.joblib\")\n",
    "\n",
    "# threshold @ ~5% FPR on OOF fused\n",
    "thr_fpr05 = float(np.quantile(oof_fused[y==0], 0.95))\n",
    "\n",
    "meta = {\n",
    "    \"dataset_rows\": int(len(y)),\n",
    "    \"label_col\": \"y\",\n",
    "    \"group_col\": \"group_id\",\n",
    "    \"text_col\": \"doc_text_fixed\",\n",
    "    \"img_col\": \"proxy_img_path\",\n",
    "    \"fusion\": {\n",
    "        \"type\": \"weighted_average\",\n",
    "        \"alpha_text\": float(best_a),     # 0.05\n",
    "        \"alpha_img\": float(1 - best_a),  # 0.95\n",
    "        \"thr_fpr05\": thr_fpr05\n",
    "    },\n",
    "    \"oof_metrics\": {\n",
    "        \"tfidf\": {\"roc_auc\": float(roc_auc_score(y, oof_txt)), \"pr_auc\": float(average_precision_score(y, oof_txt))},\n",
    "        \"img\":   {\"roc_auc\": float(roc_auc_score(y, oof_img)), \"pr_auc\": float(average_precision_score(y, oof_img))},\n",
    "        \"fused\": {\"roc_auc\": float(roc_auc_score(y, oof_fused)), \"pr_auc\": float(average_precision_score(y, oof_fused))}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(outdir/\"fusion_meta.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"✅ Saved to:\", outdir)\n",
    "print(\"thr_fpr05:\", thr_fpr05)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0qjc6d8mWPWV",
    "outputId": "ace102e2-587b-4b3e-9798-2f9f6115840f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# assumes: clip_model, clip_proc already loaded; if not, reload them:\n",
    "# clip_name = \"openai/clip-vit-base-patch32\"\n",
    "# clip_model = CLIPModel.from_pretrained(clip_name).to(device)\n",
    "# clip_proc = CLIPProcessor.from_pretrained(clip_name)\n",
    "\n",
    "alpha = float(best_a)  # 0.05\n",
    "thr = float(np.quantile(oof_fused[y==0], 0.95))\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_embed_one(img_path: str):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    inputs = clip_proc(images=[img], return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    feat = clip_model.get_image_features(**inputs)\n",
    "    feat = torch.nn.functional.normalize(feat, p=2, dim=1)\n",
    "    return feat.cpu().numpy()\n",
    "\n",
    "def fused_predict_one(doc_text_fixed: str, proxy_img_path: str):\n",
    "    # text score\n",
    "    p_txt = float(tfidf_lr.predict_proba([doc_text_fixed])[0,1])\n",
    "    # image score\n",
    "    emb = clip_embed_one(proxy_img_path)\n",
    "    p_img = float(img_lr.predict_proba(emb)[0,1])\n",
    "    # fuse\n",
    "    p = alpha * p_txt + (1-alpha) * p_img\n",
    "    return {\"p_txt\": p_txt, \"p_img\": p_img, \"p_fused\": p, \"pred\": int(p >= thr), \"thr\": thr}\n",
    "\n",
    "print(\"✅ fused_predict_one ready\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-gfGEkXWwPT",
    "outputId": "e12c2fe5-64db-40f2-dc5e-6ff20874fa50"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sanity: dataset + alignment checks"
   ],
   "metadata": {
    "id": "J4xVk9h7YHU7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Use the dataset that matches your OOF arrays (deg_mm in multimodal section)\n",
    "assert \"deg_mm\" in globals(), \"deg_mm not found. Use the dataframe you used for oof_txt/oof_img/oof_fused.\"\n",
    "y = deg_mm[\"y\"].astype(int).values\n",
    "groups = deg_mm[\"group_id\"].astype(str).values\n",
    "\n",
    "print(\"N:\", len(y), \"| y counts:\", dict(zip(*np.unique(y, return_counts=True))))\n",
    "print(\"#groups:\", len(np.unique(groups)))\n",
    "\n",
    "# Alignment checks\n",
    "for name in [\"oof_txt\", \"oof_img\", \"oof_fused\"]:\n",
    "    assert name in globals(), f\"{name} not found in globals()\"\n",
    "    arr = globals()[name]\n",
    "    assert len(arr) == len(y), f\"{name} length {len(arr)} != y length {len(y)}\"\n",
    "\n",
    "print(\"✅ OOF arrays aligned with y\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vw8JzfVHXYgL",
    "outputId": "5ad804d8-a1cd-4385-8db2-fdbc58f1b39e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Common evaluator (ROC/PR + threshold @ 5% FPR + confusion stats)"
   ],
   "metadata": {
    "id": "RQss4XMbaKNN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "def thr_at_fpr(y_true, proba, target_fpr=0.05):\n",
    "    y_true = np.asarray(y_true)\n",
    "    proba = np.asarray(proba)\n",
    "    neg = proba[y_true == 0]\n",
    "    return float(np.quantile(neg, 1 - target_fpr))\n",
    "\n",
    "def eval_probs(y_true, proba, target_fpr=0.05):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    proba = np.asarray(proba).astype(float)\n",
    "    thr = thr_at_fpr(y_true, proba, target_fpr=target_fpr)\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, pred).ravel()\n",
    "\n",
    "    fpr = fp / (fp + tn) if (fp + tn) else np.nan\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else np.nan\n",
    "    precision = tp / (tp + fp) if (tp + fp) else np.nan\n",
    "\n",
    "    return {\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, proba)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, proba)),\n",
    "        \"thr_fpr05\": float(thr),\n",
    "        \"FPR\": float(fpr),\n",
    "        \"TPR\": float(tpr),\n",
    "        \"precision@thr\": float(precision),\n",
    "        \"TN\": int(tn), \"FP\": int(fp), \"FN\": int(fn), \"TP\": int(tp),\n",
    "        \"N\": int(len(y_true))\n",
    "    }\n",
    "\n",
    "res_txt   = eval_probs(y, oof_txt,   0.05)\n",
    "res_img   = eval_probs(y, oof_img,   0.05)\n",
    "res_fused = eval_probs(y, oof_fused, 0.05)\n",
    "\n",
    "print(\"✅ eval functions ready\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olU29vKTZBz1",
    "outputId": "c40f0b42-78b7-4b4e-e926-62e9b303f175"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compare all models in one table (+ add RoBERTa holdout as separate row)\n"
   ],
   "metadata": {
    "id": "MPCDfeC-aTVF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "rows = []\n",
    "\n",
    "rows.append({\"model\":\"TFIDF-only (OOF, GroupCV)\", **res_txt})\n",
    "rows.append({\"model\":\"IMG-only CLIP+LR (OOF, GroupCV)\", **res_img})\n",
    "rows.append({\"model\":\"FUSED (OOF, GroupCV)\", **res_fused, \"alpha_text\": float(best_a) if \"best_a\" in globals() else np.nan})\n",
    "\n",
    "# Add RoBERTa holdout metrics you reported (not OOF)\n",
    "# If you have the numbers stored, fill them; otherwise keep what you typed.\n",
    "roberta_row = {\n",
    "    \"model\": \"RoBERTa FT (GroupHoldout)\",\n",
    "    \"roc_auc\": 0.8997668997668997,\n",
    "    \"pr_auc\": 0.9510900050081298,\n",
    "    \"thr_fpr05\": np.nan,\n",
    "    \"FPR\": np.nan, \"TPR\": np.nan, \"precision@thr\": np.nan,\n",
    "    \"TN\": np.nan, \"FP\": np.nan, \"FN\": np.nan, \"TP\": np.nan,\n",
    "    \"N\": np.nan\n",
    "}\n",
    "rows.append(roberta_row)\n",
    "\n",
    "cmp = pd.DataFrame(rows)\n",
    "cmp = cmp[[\"model\",\"roc_auc\",\"pr_auc\",\"FPR\",\"TPR\",\"precision@thr\",\"thr_fpr05\",\"TN\",\"FP\",\"FN\",\"TP\",\"N\"]]\n",
    "cmp = cmp.sort_values([\"pr_auc\",\"roc_auc\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "cmp\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "kanno-jjaNBl",
    "outputId": "8a9d28a5-c285-4a4f-fd08-bb5c8575af67"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "37Xw-7P9aW9A"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "authorship_tag": "ABX9TyP3sylH+ZVWwOKGe6x092f7",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}